%*******************************************************
% Prefazione 
%*******************************************************

\myChapter*{Preface}
\addtocontents{toc}{\protect\vspace{\beforebibskip}}
\addcontentsline{toc}{chapter}{\numberline{}\tocEntry{Preface}}
%\markboth{\spacedlowsmallcaps{Preface}}{\spacedlowsmallcaps{Preface}} 

%\adjustmtc
%\mtcaddchapter[\numberline{}\tocEntry{Preface}]

This preface and these notes are written from a physicist' perspective. 
This will ultimately drive the approach towards the subject and it will bias the selection of topics
presented here,
the examples discussed throughout this work, and the
level of mathematical rigour (higher than what is customary in several books of practical statistics) ,
leading to prioritize definitely what is likely to be of major relevance or more common for an
audiance of physicists, nevertheless trying to emphasize at the same time the impacts, implications 
and applications 
in contemporary information technology and big data community.

A through understanding of traditional and modern 
statistical inference (theory, methodologies, techniques, and algorithms) 
has always been playing quite a prominent  
role in the education and training of a scientist (in general) and of a
physicist (in particular), providing the framework and the tools to analyse
in a reliable and rigorous way 
the outcomes of
either actual experiments 
or virtual computer simulations,
in order to get insights, estimate parameters and their accuracy and precision, make comparisons between theory and
experiments, validate 
hypothesis tests, assess the validity of a theoretical model and its underlying
assumptions against experimental data, and make predictions. 

The need of a powerful background in statistics has probably become even more
demanding in recent years for practitioners in data analysis. 
The exponential growth of computing resources during the past few decades has made
\mynth{21} century increasingly data-intensive.
Beyond ``traditional methods (\eg, probabilitic modelling, linear and non-linear  regression theory,
confidence intervals, theory of estimators,  statistical significance,
hypothesis test, time series analysis, etc), new computer-intensive powerful
techniques has emerged in the last decades, including new progress in
Bayesian statistics, maximum entropy methods, computational statistics
(montecarlo methods, expectation-maximization algorithm,
hidden Markov models, resampling techniques such as the statistical bootstrap, etc), statistical learning 
(artificial neural
networks, support vector machines, $k$-means clustering, and
related topics in pattern recognition, etc), just
to mention few examples. 



In this respect, 
physics has always been a rich source of probabilistic models.
(statistical
mechanics, etc).
statistical mechanics (lattice spin models such as Potts models or spin glasses, ergodic theory), group
theoretical methods and concepts borrower from 
relativistic 
quantum field theory  and high energy particle physics provide valuable
insights  to better understand 
the mechanism behind these tools.

From a theoretical viewpoint,
is a beautiful and
fashinating field of research, 
sharing 
important connections with information theory, mathematics, 
artificial intelligence and robotics, physics (\eg, statistical
mechanics), etc. 
But the relevance of these methods is far from being just of theoretical
interest. 

From a practical viewpoint, statistical methods 
are undergoing a tremendus  development in recent years.
This is not only relevant for physics however.
playing an increasing widespread role  in modern information
technology and big data community, 
Warning about the misleading analysis and abuse of some statistical technique,
nevertheless 
helping in this way to popularize some cornerstone
machine learning techniques. 
while the size of data is posing challanging questions on how to efficiently
pro
and they 
have proved successful to approach data analysis in many
different areas, not only including physics, 
impacting 

Pervasiveness of distributed cloud-based clusters, crucially providing both the 
computational and the storage resources necessary for processing massive datasets;

The data-driven paradigm is emerging, the internet of things (IoT) is promising
an even more 
increasing  volume of 
data
to be digested in the upcoming years, 
pressing for successful strategies capable of 
processing at high rate (or even in real-time) massive distributed datasets. 

Mastering these topics requires a mix of knowledge, skills and experience to
succeed, together with high proficiency with related algorithms and their
implementation. 
Familiarity with algorithms
and ability to efficiently implement them 
is an essential part, 
and it can become even less trivial when the data requires distributed or
high-performance computing.


Two schools: frequentist and Bayesian.
Unfortunately, there is no general consensus among statisticians (and even
among physicists)

The aim of this notes is slightly ambitious.
First, thoroughly  discuss at  graduate level the 
 theoretical underpinnings of traditional and modern statistical tools and
 their 
 mathematical foundations, establishing the main results and developing theoretical
 tools.
 I firmly believe that 
 it is not possible to perform a reliable data analysis without a solid
 knowledge of the theory behind those tools: theory is essential to understand and control the
range of  validity of a given method, to understand how to inspect the results,
and to highlight artifacts (which are always
present), and in case to develop custum solutions for the problem at hand. 
In the second part, we will discuss concrete examples, together with actual implementation of algorithms to handle data
analysis in real examples. 
This will help to illustrate pitfalls, best practises, implementation
strategies, libraries and platforms, etc


		\vskip2ex

\mySignature{\myLocation, \myTime}{\Large{\calligra\myName}}
